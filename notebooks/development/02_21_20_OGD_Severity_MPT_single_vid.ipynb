{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the terms of the experiment are defined, such as the location of the files in S3 (bucket and folder name), and each of the video prefixes (everything before the file extension) that need to be tracked. \n",
    "\n",
    "Note that these videos should be similar-ish: while we can account for differences in mean intensities between videos, particle sizes should be approximately the same, and (slightly less important) particles should be moving at about the same order of magnitude speed. In this experiment, these videos were taken in 0.4% agarose gel at 100x magnification and 100.02 fps shutter speeds with nanoparticles of about 100nm in diameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track = [] # This is going to be the list of all filenames that will be included in the analysis\n",
    "start_knot = 75 #Must be unique number for every run on Cloudknot.\n",
    "\n",
    "remote_folder = '02_21_20_OGD_Severity_MPT' # The folder in AWS S3 containing the files to be analyzed\n",
    "bucket = 'mckenna.data' # The bucket in AWS S3 where the remote_folder is contained\n",
    "vids = 5 # this is the number of vids that were taken per condition (usually corresponding to different locations)\n",
    "conditions = ['NT', 'OGD_3h']\n",
    "NT_slices = 2\n",
    "OGD_3h_slices = 3\n",
    "NT_regions = ['cortex', 'hippocampus', 'striatum']\n",
    "OGD_3h_regions = ['cortex', 'striatum']\n",
    "for cond in conditions:\n",
    "    if cond == 'NT':\n",
    "        for slic in range(1,NT_slices+1):\n",
    "            if slic == 1:\n",
    "                for reg in NT_regions:\n",
    "                    if reg == 'cortex':\n",
    "                        for num in range(1,11):\n",
    "                            to_track.append('{}_slice_{}_{}_vid_{}'.format(cond, slic, reg, num))\n",
    "                    elif reg == 'hippocampus':\n",
    "                        for num in range(1,7):\n",
    "                            to_track.append('{}_slice_{}_{}_vid_{}'.format(cond, slic, reg, num))\n",
    "                    else:\n",
    "                        for num in range(1,vids+1):\n",
    "                            to_track.append('{}_slice_{}_{}_vid_{}'.format(cond, slic, reg, num))\n",
    "            else:\n",
    "                for reg in NT_regions:\n",
    "                    for num in range(1,vids+1):\n",
    "                        to_track.append('{}_slice_{}_{}_vid_{}'.format(cond, slic, reg, num))\n",
    "    else:\n",
    "        for slic in range(1,OGD_3h_slices+1):\n",
    "            for reg in OGD_3h_regions:\n",
    "                for num in range(1,vids+1):\n",
    "                    to_track.append('{}_slice_{}_{}_vid_{}'.format(cond, slic, reg, num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NT_slice_1_cortex_vid_1',\n",
       " 'NT_slice_1_cortex_vid_2',\n",
       " 'NT_slice_1_cortex_vid_3',\n",
       " 'NT_slice_1_cortex_vid_4',\n",
       " 'NT_slice_1_cortex_vid_5',\n",
       " 'NT_slice_1_cortex_vid_6',\n",
       " 'NT_slice_1_cortex_vid_7',\n",
       " 'NT_slice_1_cortex_vid_8',\n",
       " 'NT_slice_1_cortex_vid_9',\n",
       " 'NT_slice_1_cortex_vid_10',\n",
       " 'NT_slice_1_hippocampus_vid_1',\n",
       " 'NT_slice_1_hippocampus_vid_2',\n",
       " 'NT_slice_1_hippocampus_vid_3',\n",
       " 'NT_slice_1_hippocampus_vid_4',\n",
       " 'NT_slice_1_hippocampus_vid_5',\n",
       " 'NT_slice_1_hippocampus_vid_6',\n",
       " 'NT_slice_1_striatum_vid_1',\n",
       " 'NT_slice_1_striatum_vid_2',\n",
       " 'NT_slice_1_striatum_vid_3',\n",
       " 'NT_slice_1_striatum_vid_4',\n",
       " 'NT_slice_1_striatum_vid_5',\n",
       " 'NT_slice_2_cortex_vid_1',\n",
       " 'NT_slice_2_cortex_vid_2',\n",
       " 'NT_slice_2_cortex_vid_3',\n",
       " 'NT_slice_2_cortex_vid_4',\n",
       " 'NT_slice_2_cortex_vid_5',\n",
       " 'NT_slice_2_hippocampus_vid_1',\n",
       " 'NT_slice_2_hippocampus_vid_2',\n",
       " 'NT_slice_2_hippocampus_vid_3',\n",
       " 'NT_slice_2_hippocampus_vid_4',\n",
       " 'NT_slice_2_hippocampus_vid_5',\n",
       " 'NT_slice_2_striatum_vid_1',\n",
       " 'NT_slice_2_striatum_vid_2',\n",
       " 'NT_slice_2_striatum_vid_3',\n",
       " 'NT_slice_2_striatum_vid_4',\n",
       " 'NT_slice_2_striatum_vid_5',\n",
       " 'OGD_3h_slice_1_cortex_vid_1',\n",
       " 'OGD_3h_slice_1_cortex_vid_2',\n",
       " 'OGD_3h_slice_1_cortex_vid_3',\n",
       " 'OGD_3h_slice_1_cortex_vid_4',\n",
       " 'OGD_3h_slice_1_cortex_vid_5',\n",
       " 'OGD_3h_slice_1_striatum_vid_1',\n",
       " 'OGD_3h_slice_1_striatum_vid_2',\n",
       " 'OGD_3h_slice_1_striatum_vid_3',\n",
       " 'OGD_3h_slice_1_striatum_vid_4',\n",
       " 'OGD_3h_slice_1_striatum_vid_5',\n",
       " 'OGD_3h_slice_2_cortex_vid_1',\n",
       " 'OGD_3h_slice_2_cortex_vid_2',\n",
       " 'OGD_3h_slice_2_cortex_vid_3',\n",
       " 'OGD_3h_slice_2_cortex_vid_4',\n",
       " 'OGD_3h_slice_2_cortex_vid_5',\n",
       " 'OGD_3h_slice_2_striatum_vid_1',\n",
       " 'OGD_3h_slice_2_striatum_vid_2',\n",
       " 'OGD_3h_slice_2_striatum_vid_3',\n",
       " 'OGD_3h_slice_2_striatum_vid_4',\n",
       " 'OGD_3h_slice_2_striatum_vid_5',\n",
       " 'OGD_3h_slice_3_cortex_vid_1',\n",
       " 'OGD_3h_slice_3_cortex_vid_2',\n",
       " 'OGD_3h_slice_3_cortex_vid_3',\n",
       " 'OGD_3h_slice_3_cortex_vid_4',\n",
       " 'OGD_3h_slice_3_cortex_vid_5',\n",
       " 'OGD_3h_slice_3_striatum_vid_1',\n",
       " 'OGD_3h_slice_3_striatum_vid_2',\n",
       " 'OGD_3h_slice_3_striatum_vid_3',\n",
       " 'OGD_3h_slice_3_striatum_vid_4',\n",
       " 'OGD_3h_slice_3_striatum_vid_5']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The videos used with this analysis are fairly large (2048 x 2048 pixels and 651 frames), and in cases like this, the tracking algorithm can quickly eat up RAM. In this case, we chose to crop the videos to 512 x 512 images such that we can run our jobs on smaller EC2 instances with 16GB of RAM. \n",
    "\n",
    "Note that larger jobs can be made with user-defined functions such that splitting isn't necessary-- or perhaps an intermediate amount of memory that contains splitting, tracking, and msd calculation functions all performed on a single EC2 instance.\n",
    "\n",
    "The compiled functions in the knotlets module require access to buckets on AWS. In this case, we will be using a publicly (read-only) bucket. If users want to run this notebook on their own, will have to transfer files from nancelab.publicfiles to their own bucket, as it requires writing to S3 buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diff_classifier.knotlets as kn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell uses the function kn.split() to split all of the videos contained in 'to_track' into 16 smaller videos on which the actual tracking will be performed\n",
    "for prefix in to_track:\n",
    "    kn.split(prefix, remote_folder=remote_folder, bucket=bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking normally requires user input in the form of tracking parameters e.g. particle radius, linking max distance, max frame gap etc. When large datasets aren't required, each video can be manageably manually tracked using the TrackMate GUI. However, when datasets get large e.g. >20 videos, this can become extremely arduous. For videos that are fairly similar, you can get away with using similar tracking parameters across all videos. However, one parameter that is a little more noisy that the others is the quality filter value. Quality is a numerical value that approximate how likely a particle is to be \"real.\" \n",
    "\n",
    "In this case, I built a predictor that estimates the quality filter value based on intensity distributions from the input images. Using a relatively small training dataset (5-20 videos), users can get fairly good estimates of quality filter values that can be used in parallelized tracking workflows.\n",
    "\n",
    "Note: in the current setup, the predictor should be run in Python 3. While the code will run in Python 3, there are differences between the random number generators in Python2 and Python3 that I was not able to control for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import diff_classifier.imagej as ij\n",
    "import boto3\n",
    "import os.path as op\n",
    "import diff_classifier.aws as aws\n",
    "import diff_classifier.knotlets as kn\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regress_sys function should be run twice. When have_output is set to False, it generates a list of files that the user should manually track using Trackmate. Once the quality filter values are found, they can be used as input (y) to generate a regress object that can predict quality filter values for additional videos. Once y is assigned, set have_output to True and re-run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnum=30 #number of training datasets\n",
    "pref = []\n",
    "for num in to_track:                    \n",
    "    for row in range(0, 4):\n",
    "        for col in range(0, 4):\n",
    "            pref.append(\"{}_{}_{}\".format(num, row, col))\n",
    "\n",
    "y = np.array([2.67, 2.11, 4.09, 5.15, 7.75, 4.25, 5.24, 2.75, 3.34, 7.44, 2.13, 1.89, 9.37, 7.07, 3.01, 6.33, 6.42, 6.44, 2.74, 2.85, 26.36, 2.79, 3.59, 5.07, 5.42, 5.95, 6.70, 3.44, 2.27, 4.27])\n",
    "\n",
    "# Creates regression object based of training dataset composed of input images and manually\n",
    "# calculated quality cutoffs from tracking with GUI interface.\n",
    "regress = ij.regress_sys(remote_folder, pref, y, tnum, randselect=True,\n",
    "                         have_output=True, bucket_name=bucket)\n",
    "#Read up on how regress_sys works before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "print(len(to_track))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickle object\n",
    "filename = 'regress.obj'\n",
    "with open(filename,'wb') as fp:\n",
    "    joblib.dump(regress,fp)\n",
    "\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "aws.upload_s3(filename, remote_folder+'/'+filename, bucket_name=bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users should input all tracking parameters into the tparams object. Note that the quality value will be overwritten by values found using the quality predictor found above. Never change threshold, median intensity, or snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tparams1 = {'radius': 6.0, 'threshold': 0.0, 'do_median_filtering': False,\n",
    "           'quality': 10.0, 'xdims': (0, 511), 'ydims': (1, 511),\n",
    "           'median_intensity': 300.0, 'snr': 0.0, 'linking_max_distance': 12.0,\n",
    "           'gap_closing_max_distance': 15.0, 'max_frame_gap': 8.0,\n",
    "           'track_duration': 10.00}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempting to run a single vid at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_folder = '02_21_20_OGD_Severity_MPT'\n",
    "bucket = 'mckenna.data'\n",
    "filename = 'NT_slice_1_striatum_vid_2_0_3.tif'\n",
    "aws.download_s3(remote_folder+'/'+filename, filename, bucket_name=bucket)\n",
    "local_name = filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op.exists('/home/ubuntu/source/mike_fork/diff_classifier/notebooks/development')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_path = '/home/ubuntu/source/mike_fork/diff_classifier/notebooks/development/'\n",
    "target = vid_path+local_name\n",
    "out_csv = 'Traj_'+local_name+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'linux2'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/source/mike_fork/diff_classifier/notebooks/development/NT_slice_1_striatum_vid_2_0_3.tif\n",
      "Traj_NT_slice_1_striatum_vid_2_0_3.tif.csv\n"
     ]
    }
   ],
   "source": [
    "print(target)\n",
    "print(out_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'track_displacement'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-20aba2aa25d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                                      \u001b[0;34m'xdims'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m511\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ydims'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m511\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'median_intensity'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m300.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                                      \u001b[0;34m'snr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'linking_max_distance'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m15.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gap_closing_max_distance'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m18.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                                                      'max_frame_gap': 8, 'track_duration':20.0})\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/source/diff-classifier/diff_classifier/imagej.pyc\u001b[0m in \u001b[0;36mtrack\u001b[0;34m(target, out_file, template, fiji_bin, tparams)\u001b[0m\n\u001b[1;32m    176\u001b[0m                             \u001b[0mgap_closing_max_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gap_closing_max_distance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                             \u001b[0mmax_frame_gap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_frame_gap'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                             track_duration=str(tparams['track_duration'])))\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s --ij2 --headless --run %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfiji_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'track_displacement'"
     ]
    }
   ],
   "source": [
    "ij.track(local_name, out_csv, template=None, fiji_bin=None, tparams={'radius': 6.0, 'threshold': 0.0, \n",
    "                                                                     'quality': 2.67, 'do_median_filtering': False, \n",
    "                                                                     'xdims':(0,511), 'ydims':(1,511), 'median_intensity': 300.0,\n",
    "                                                                     'snr':0.0, 'linking_max_distance': 15.0, 'gap_closing_max_distance': 18.0, \n",
    "                                                                     'max_frame_gap': 8, 'track_duration':10.0})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## second attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diff_classifier.knotlets as kn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'NT_slice_1_striatum_vid_2_0_3'\n",
    "remote_folder = '02_21_20_OGD_Severity_MPT'\n",
    "s3_bucket = 'mckenna.data'\n",
    "tparams1={'radius': 6.0, 'threshold': 0.0, 'quality': 2.67, 'do_median_filtering': False,\n",
    "         'xdims':(0,511), 'ydims':(1,511), 'median_intensity': 300.0,\n",
    "         'snr':0.0, 'linking_max_distance': 15.0, 'gap_closing_max_distance': 18.0,\n",
    "         'max_frame_gap': 8, 'track_duration':10.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'track_displacement'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-43c7d01b7ac7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m kn.tracking(prefix, remote_folder, bucket=s3_bucket, regress_f='regress.obj', rows=4, cols=4, ires=(512,512),\n\u001b[0;32m----> 2\u001b[0;31m             tparams=tparams1)\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/source/diff-classifier/diff_classifier/knotlets.pyc\u001b[0m in \u001b[0;36mtracking\u001b[0;34m(subprefix, remote_folder, bucket, regress_f, rows, cols, ires, tparams)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         ij.track(local_im, outfile, template=None, fiji_bin=None,\n\u001b[0;32m--> 144\u001b[0;31m                  tparams=tparams)\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0maws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_s3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremote_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     print(\"Done with tracking.  Should output file of name {}\".format(\n",
      "\u001b[0;32m/home/ubuntu/source/diff-classifier/diff_classifier/imagej.pyc\u001b[0m in \u001b[0;36mtrack\u001b[0;34m(target, out_file, template, fiji_bin, tparams)\u001b[0m\n\u001b[1;32m    176\u001b[0m                             \u001b[0mgap_closing_max_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gap_closing_max_distance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                             \u001b[0mmax_frame_gap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_frame_gap'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                             track_duration=str(tparams['track_duration'])))\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s --ij2 --headless --run %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfiji_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'track_displacement'"
     ]
    }
   ],
   "source": [
    "kn.tracking(prefix, remote_folder, bucket=s3_bucket, regress_f='regress.obj', rows=4, cols=4, ires=(512,512),\n",
    "            tparams=tparams1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloudknot setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloudknot requires the user to define a function that will be sent to multiple computers to run. In this case, the function knotlets.tracking will be used. We create a docker image that has the required installations (defined by the requirements.txt file from diff_classifier on Github, and the base Docker Image below that has Fiji pre-installed in the correct location.\n",
    "\n",
    "Note that I modify the Docker image below such that the correct version of boto3 is installed. For some reason, versions later than 1.5.28 error out, so I specified 5.28 as the correct version. Run my_image.build below to double-check that the Docker image is successfully built prior to submitting the job to Cloudknot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Before you run this next cell, you have to switch the kernel from Python 3 to Python 2, by doing the following: **\n",
    " 1. Kernel -> Restart and clear output\n",
    " 2. Kernel -> Change Kernel -> Python 2\n",
    " 3. Rerun cells required to run below cell\n",
    " \n",
    " ** One other important thing to note: \n",
    " - If you are performing the tracking, be sure that the my_image =  line is set to ck.DockerImage(func=kn.tracking,...\n",
    " - If you are performing the MSD/feature calculation (after you've carried out the tracking), be sure that the my_image = line is se to ck.DockerImage(func=kn.assemble_msds, ...\n",
    "     \n",
    "     ** following the tracking, before you run assemble_msds, you need to run the cell below that redefines all_maps as all_maps2. all_maps2 doesn't include the tparams1 input, and allows the kn.assemble_msds section to run properly. It won't work with the tparams input. **\n",
    "     \n",
    " Other than that, everything else should stay the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudknot as ck\n",
    "import os.path as op\n",
    "\n",
    "github_installs=('https://github.com/ccurtis7/diff_classifier.git')\n",
    "my_image = ck.DockerImage(func=kn.tracking, base_image='arokem/python3-fiji:0.3', github_installs=github_installs)\n",
    "#my_image = ck.DockerImage(func=kn.assemble_msds, base_image='arokem/python3-fiji:0.3', github_installs=github_installs)\n",
    "docker_file = open(my_image.docker_path)\n",
    "docker_string = docker_file.read()\n",
    "docker_file.close()\n",
    "\n",
    "req = open(op.join(op.split(my_image.docker_path)[0], 'requirements.txt'))\n",
    "req_string = req.read()\n",
    "req.close()\n",
    "\n",
    "new_req = req_string[0:req_string.find('\\n')-5]+'5.28'+ req_string[req_string.find('\\n'):]\n",
    "req_overwrite = open(op.join(op.split(my_image.docker_path)[0], 'requirements.txt'), 'w')\n",
    "req_overwrite.write(new_req)\n",
    "req_overwrite.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the execution of this cell, you have to check that the requirements.txt file has the first line 'boto3==1.5.28'.\n",
    "    - This file can be found in source -> diff-classifier -> notebooks -> development -> most recent file\n",
    "\n",
    "If it doesn't, you may have to change the line in the cell above that says 'new_req = reg_string[0:req_string.find('\\n')-4]+'5.28'+ reg_string[reg_string.find('\\n'):]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_image.build(\"ChABC_slice_2\", image_name=\"test_image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object all_maps is an iterable containing all the inputs sent to Cloudknot. This is useful, because if the user needs to modify some of the tracking parameters for a single video, this can be done prior to submission to Cloudknot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "all_maps = []\n",
    "for prefix in to_track[0:10]:    \n",
    "    for i in range(0, 4):\n",
    "        for j in range(0, 4):\n",
    "            names.append('{}_{}_{}'.format(prefix, i, j))\n",
    "            all_maps.append(('{}_{}_{}'.format(prefix, i, j), remote_folder, bucket, 'regress.obj', 4, 4, (512, 512), tparams1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_knot = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cloudknot knot object sets up the compute environment which will run the code. Note that the name must be unique. Every time you submit a new knot, you should change the name. I do this with the variable start_knot, which I vary for each run.\n",
    "\n",
    "If larger jobs are anticipated, users can adjust both RAM and storage with the memory and image_id variables. Memory specifies the amount of RAM to be used. Users can build a customized AMI with as much space as they need, and enter the ID into image_ID. Read the Cloudknot documentation for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RetryError",
     "evalue": "RetryError[<Future at 0x7f2b5cc74610 state=finished raised NoSuchEntityException>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-db8df215f1e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                \u001b[0mbid_percentage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                \u001b[0;31m#image_id = 'ami-0e00afdf500081a0d', #May need to change this line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                pars_policies=('AmazonS3FullAccess',))\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mresult_futures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_maps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/source/cloudknot/cloudknot/cloudknot.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, pars, pars_policies, docker_image, base_image, func, image_script_path, image_work_dir, image_github_installs, username, repo_name, image_tags, job_definition_name, job_def_vcpus, memory, retries, compute_environment_name, instance_types, resource_type, min_vcpus, max_vcpus, desired_vcpus, image_id, ec2_key_pair, ce_tags, bid_percentage, job_queue_name, priority)\u001b[0m\n\u001b[1;32m   1337\u001b[0m             )\n\u001b[1;32m   1338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1339\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpars_cleanup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pars'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m             futures['compute-environment'] = executor.submit(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/concurrent/futures/_base.pyc\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/concurrent/futures/thread.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/source/cloudknot/cloudknot/cloudknot.pyc\u001b[0m in \u001b[0;36mset_pars\u001b[0;34m(knot_name, input_pars, pars_policies)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                     \u001b[0mpars_cleanup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                     \u001b[0mpars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mknot_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpars_policies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                     mod_logger.info('knot {name:s} created PARS {p:s}'.format(\n",
      "\u001b[0;32m/home/ubuntu/source/cloudknot/cloudknot/cloudknot.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, batch_service_role_name, ecs_instance_role_name, ecs_task_role_name, spot_fleet_role_name, policies, vpc_id, vpc_name, use_default_vpc, security_group_id, security_group_name)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ecs_instance_role\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ecs_instance_role'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ecs_task_role\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ecs_task_role'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spot_fleet_role\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spot_fleet_role'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vpc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_security_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vpc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/concurrent/futures/_base.pyc\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/concurrent/futures/thread.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/source/cloudknot/cloudknot/cloudknot.pyc\u001b[0m in \u001b[0;36mset_role\u001b[0;34m(pars_name, role_name, service, policies, add_instance_profile)\u001b[0m\n\u001b[1;32m    321\u001b[0m                         \u001b[0mservice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                         \u001b[0mpolicies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                         \u001b[0madd_instance_profile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_instance_profile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m                     )\n\u001b[1;32m    325\u001b[0m                     mod_logger.info('PARS {name:s} created role {role:s}'\n",
      "\u001b[0;32m/home/ubuntu/source/cloudknot/cloudknot/aws/iam.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, description, service, policies, add_instance_profile)\u001b[0m\n\u001b[1;32m    153\u001b[0m                                           'boolean input')\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_instance_profile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_instance_profile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0m_allowed_services\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ec2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ecs-tasks'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lambda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'spotfleet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/source/cloudknot/cloudknot/aws/iam.pyc\u001b[0m in \u001b[0;36m_create\u001b[0;34m(self, add_instance_profile)\u001b[0m\n\u001b[1;32m    303\u001b[0m             retry.call(\n\u001b[1;32m    304\u001b[0m                 \u001b[0mclients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iam'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattach_role_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                 \u001b[0mPolicyArn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_arn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRoleName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m             )\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tenacity/__init__.pyc\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             do = self.iter(result=result, exc_info=exc_info,\n\u001b[0;32m--> 295\u001b[0;31m                            start_time=start_time)\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tenacity/__init__.pyc\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, result, exc_info, start_time)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/six.pyc\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRetryError\u001b[0m: RetryError[<Future at 0x7f2b5cc74610 state=finished raised NoSuchEntityException>]"
     ]
    }
   ],
   "source": [
    "knot = ck.Knot(name='{}_d{}'.format('mike', start_knot),\n",
    "               docker_image = my_image,\n",
    "               memory = 16000,\n",
    "               resource_type = \"SPOT\",\n",
    "               bid_percentage = 100,\n",
    "               #image_id = 'ami-0e00afdf500081a0d', #May need to change this line\n",
    "               pars_policies=('AmazonS3FullAccess',))\n",
    "\n",
    "result_futures = knot.map(all_maps, starmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot.clobber()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can track the progression of your run using the AWS Batch service online -- make sure you are looking at the right US Region.\n",
    "\n",
    "After the run, you might have some that fail. This usually happens when the computers get claimed by someone paying more money, and your job gets booted from the aws computers. Because of this, you will need to start a knw cloudknot knot and rerun those vids. The set up for that is shown below.\n",
    "\n",
    "Remember to clobber your knot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck.get_region()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a new all_maps2 array for any of the videos that failed to get analyzed the first time through. Double check that it worked well by printing the length of it immediately afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = []\n",
    "all_maps2 = []\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "\n",
    "for name in names:\n",
    "    try:\n",
    "        s3.Object(bucket, '{}/Traj_{}.csv'.format(remote_folder, name)).load()\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            missing.append(name)\n",
    "            all_maps2.append((name, remote_folder, bucket, 'regress.obj',\n",
    "                             4, 4, (512, 512), tparams1))\n",
    "        else:\n",
    "            print('Something else has gone wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_maps2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you change the name of your knot, either by changing the 'mike1' part or start_knot value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_maps2 = []\n",
    "for prefix in to_track:\n",
    "    all_maps2.append((prefix, remote_folder, bucket, (512, 512), 651, 4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can monitor the progress of their job in the Batch interface. Once the code is complete, users should clobber their knot to make sure that all AWS resources are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream analysis and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The knotlet.assemble_msds function (which can also potentially be submitted to Cloudknot as well for large jobs) calculates the mean squared displacements and trajectory features from the raw trajectory csv files found from the Cloudknot submission. It accesses them from the S3 bucket to which they were saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prefix in to_track:\n",
    "    kn.assemble_msds(prefix, remote_folder, bucket='mckenna.data')\n",
    "    print('Successfully output msds for {}'.format(prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prefix in to_track[5:7]:\n",
    "    kn.assemble_msds(prefix, remote_folder, bucket='ccurtis.data')\n",
    "    print('Successfully output msds for {}'.format(prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diff_classifier includes some useful imaging tools as well, including checking trajectories, plotting heatmaps of trajectory features, distributions of diffusion coefficients, and MSD plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diff_classifier.heatmaps as hm\n",
    "import diff_classifier.aws as aws\n",
    "import os\n",
    "import os.path as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vids in to_track[45:90]:\n",
    "    prefix = vids\n",
    "    msds = 'msd_{}.csv'.format(prefix)\n",
    "    feat = 'features_{}.csv'.format(prefix)\n",
    "    aws.download_s3('{}/{}'.format(remote_folder, msds), msds, bucket_name=bucket)\n",
    "    aws.download_s3('{}/{}'.format(remote_folder, feat), feat, bucket_name=bucket)\n",
    "    hm.plot_trajectories(prefix, remote_folder=remote_folder, upload=True, figsize=(8, 8), bucket = bucket)\n",
    "    print('Successfully uploaded trajectory plot for {}'.format(prefix))\n",
    "    geomean, geoSEM = hm.plot_individual_msds(prefix, x_range=4, y_range=0.5, umppx=0.07, fps=100, upload=True, remote_folder=remote_folder, bucket = bucket)\n",
    "    aws.upload_s3('./geomean_{}.csv'.format(prefix), remote_folder+'/geomean_{}.csv'.format(prefix), bucket_name = bucket)\n",
    "    aws.upload_s3('./geoSEM_{}.csv'.format(prefix), remote_folder+'/geoSEM_{}.csv'.format(prefix), bucket_name = bucket)\n",
    "    aws.upload_s3('./msds_{}.png'.format(prefix), remote_folder+'/msds_{}.png'.format(prefix), bucket_name = bucket)\n",
    "    os.remove('features_{}.csv'.format(prefix))\n",
    "    os.remove('geoSEM_{}.csv'.format(prefix))\n",
    "    os.remove('msd_{}.csv'.format(prefix))\n",
    "    os.remove('geomean_{}.csv'.format(prefix))\n",
    "    os.remove('msds_{}.png'.format(prefix))\n",
    "    print('Successfully uploaded csv files for {}'.format(prefix))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geomean, geoSEM = hm.plot_individual_msds(prefix, x_range=4, y_range=0.5, umppx=0.07, fps=100, upload=True, remote_folder=remote_folder, bucket = bucket)\n",
    "aws.upload_s3('./geomean_{}.csv'.format(prefix), remote_folder+'/geomean_{}.csv'.format(prefix), bucket_name = bucket)\n",
    "aws.upload_s3('./geoSEM_{}.csv'.format(prefix), remote_folder+'/geoSEM_{}.csv'.format(prefix), bucket_name = bucket)\n",
    "aws.upload_s3('./msds_{}.png'.format(prefix), remote_folder+'/msds_{}.png'.format(prefix), bucket_name = bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm.plot_heatmap(prefix, upload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm.plot_particles_in_frame(prefix, y_range=500, upload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = to_track[0]\n",
    "\n",
    "msds = 'msd_{}.csv'.format(prefix)\n",
    "feat = 'features_{}.csv'.format(prefix)\n",
    "aws.download_s3('{}/{}'.format(remote_folder, msds), msds, bucket_name=bucket)\n",
    "aws.download_s3('{}/{}'.format(remote_folder, feat), feat, bucket_name=bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm.plot_trajectories(prefix, remote_folder=remote_folder, upload=True, figsize=(8, 8), bucket = bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geomean, geoSEM = hm.plot_individual_msds(prefix, x_range=4, y_range=0.5, umppx=0.07, fps=100, upload=True, remote_folder=remote_folder, bucket = bucket)\n",
    "aws.upload_s3('./geomean_{}.csv'.format(prefix), remote_folder+'/geomean_{}.csv'.format(prefix), bucket_name = bucket)\n",
    "aws.upload_s3('./geoSEM_{}.csv'.format(prefix), remote_folder+'/geoSEM_{}.csv'.format(prefix), bucket_name = bucket)\n",
    "aws.upload_s3('./msds_{}.png'.format(prefix), remote_folder+'/msds_{}.png'.format(prefix), bucket_name = bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting msd files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import diff_classifier.aws as aws\n",
    "import math\n",
    "import os\n",
    "import os.path as op\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track = [] # This is going to be the list of all filenames that will be included in the analysis\n",
    "start_knot = 80 #Must be unique number for every run on Cloudknot.\n",
    "\n",
    "remote_folder = '08_06_19_MPT_age_dependence' # The folder in AWS S3 containing the files to be analyzed\n",
    "bucket = 'mckenna.data' # The bucket in AWS S3 where the remote_folder is contained\n",
    "NP_sizes = ['40','100']\n",
    "vids = 5 # this is the number of vids that were taken per condition (usually corresponding to different locations)\n",
    "ages = ['P14', 'P21', 'P28']\n",
    "slices = 3\n",
    "for size in NP_sizes:\n",
    "    for age in ages:\n",
    "        for slic in range(1, slices+1):\n",
    "            for num in range(1, vids+1):\n",
    "                to_track.append('{}_{}nm_s{}_v{}'.format(age, size, slic, num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import diff_classifier.aws as aws\n",
    "import math\n",
    "\n",
    "for prefix in to_track[45:90]:\n",
    "    filename = 'geomean_{}.csv'.format(prefix)\n",
    "    aws.download_s3(remote_folder+'/'+filename, filename, bucket_name=bucket)\n",
    "    local_name = filename\n",
    "    merged = pd.read_csv(local_name)\n",
    "    merged.columns = ['log']\n",
    "    merged['exp'] = 0\n",
    "    for rows in range(0,len(merged)):\n",
    "        log_value = merged['log'].iloc[rows]\n",
    "        exp_value = math.exp(log_value)\n",
    "        merged.loc[rows,'exp'] = exp_value\n",
    "    merged.to_csv('adj_'+filename, mode='w', index = False)\n",
    "    aws.upload_s3('./adj_'+filename, remote_folder+'/adj_'+filename, bucket_name = bucket)\n",
    "    os.remove(filename)\n",
    "    os.remove('adj_'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref = 'adj_geomean_'\n",
    "\n",
    "mol_wts = ['high', 'low', 'med']\n",
    "num_vids = 5\n",
    "\n",
    "for MW in mol_wts:\n",
    "    avg_df = pd.DataFrame()\n",
    "    for vid in range(1, num_vids+1):\n",
    "        exp = pd.read_csv(pref+'{}MW_40nm_vid_{}.csv'.format(MW, str(vid)))\n",
    "        avg_df = pd.concat([avg_df, exp['exp']], axis=1)\n",
    "    avg_df = avg_df.mean(axis = 1)\n",
    "    avg_df.to_csv('avg_'+pref+'{}MW_40nm.csv'.format(MW), mode='w', index = False)\n",
    "    aws.upload_s3('./avg_'+pref+'{}MW_40nm.csv'.format(MW), remote_folder+'/avg_'+pref+'{}MW_40nm.csv'.format(MW), bucket_name=bucket)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import diff_classifier.aws as aws\n",
    "import math\n",
    "import os\n",
    "import os.path as op\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import diff_classifier.aws as aws\n",
    "import math\n",
    "\n",
    "high_track = to_track[0:5]\n",
    "low_track = to_track[5:10]\n",
    "med_track = to_track[10:15]\n",
    "\n",
    "low_MW = pd.DataFrame()\n",
    "med_MW = pd.DataFrame()\n",
    "high_MW = pd.DataFrame()\n",
    "\n",
    "um_px = 0.07\n",
    "fps = 33\n",
    "\n",
    "for prefix in low_track:\n",
    "    feat = 'features_{}.csv'.format(prefix)\n",
    "    aws.download_s3(remote_folder+'/'+feat, 'trial_2_'+feat, bucket_name=bucket)\n",
    "    merged = pd.read_csv('trial_2_'+feat)\n",
    "    low_MW = pd.concat([low_MW, merged['Deff1']*um_px*um_px*fps/10], axis=0)\n",
    "    \n",
    "for prefix in med_track:\n",
    "    feat = 'features_{}.csv'.format(prefix)\n",
    "    aws.download_s3(remote_folder+'/'+feat, 'trial_2_'+feat, bucket_name=bucket)\n",
    "    merged = pd.read_csv('trial_2_'+feat)\n",
    "    med_MW = pd.concat([med_MW, merged['Deff1']*um_px*um_px*fps/10], axis=0)\n",
    "    \n",
    "for prefix in high_track:\n",
    "    feat = 'features_{}.csv'.format(prefix)\n",
    "    aws.download_s3(remote_folder+'/'+feat, 'trial_2_'+feat, bucket_name=bucket)\n",
    "    merged = pd.read_csv('trial_2_'+feat)\n",
    "    high_MW = pd.concat([high_MW, merged['Deff1']*um_px*um_px*fps/10], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "low_no_nan = low_MW.replace(0,np.nan)\n",
    "med_no_nan = med_MW.replace(0,np.nan)\n",
    "high_no_nan = high_MW.replace(0,np.nan)\n",
    "\n",
    "\n",
    "log_Deff_low = np.log(low_no_nan[0].dropna())\n",
    "#print(log_Deff_low)\n",
    "log_Deff_med = np.log(med_no_nan[0].dropna())\n",
    "#print(log_Deff_low)\n",
    "log_Deff_high = np.log(high_no_nan[0].dropna())\n",
    "\n",
    "test_bins = np.linspace(-12, 0, 76)\n",
    "\n",
    "low_hist, low_bins = np.histogram(log_Deff_low, bins=test_bins)\n",
    "med_hist, med_bins = np.histogram(log_Deff_med, bins=test_bins)\n",
    "high_hist, high_bins = np.histogram(log_Deff_high, bins=test_bins)\n",
    "\n",
    "low_avg = np.mean(log_Deff_low)\n",
    "med_avg = np.mean(log_Deff_med)\n",
    "high_avg = np.mean(log_Deff_high)\n",
    "\n",
    "plt.rc('axes', linewidth=2)\n",
    "low_plot, med_plot, high_plot = low_hist, med_hist, high_hist\n",
    "bins = test_bins\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:])/2\n",
    "\n",
    "plt.figure(1, figsize=(7,4)).tight_layout()\n",
    "plt.bar(center, high_plot, color='royalblue', align='center', width=width, alpha=0.6, label='high MW')\n",
    "plt.axvline(high_avg, color='royalblue', linestyle='--', linewidth=3)\n",
    "plt.ylim((0,800))\n",
    "plt.xlabel('log $D_{eff}$', fontsize=20)\n",
    "plt.ylabel('Trajectory Count', fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.figure(1, figsize=(7,4)).tight_layout()\n",
    "plt.bar(center, med_plot, color='darkgreen', align='center', width=width, alpha=0.7, label='medium MW')\n",
    "plt.axvline(med_avg, color='darkgreen', linestyle='--', linewidth=3)\n",
    "plt.ylim((0,400))\n",
    "plt.xlabel('log $D_{eff}$', fontsize=20)\n",
    "plt.ylabel('Trajectory Count', fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.figure(1, figsize=(7,4)).tight_layout()\n",
    "plt.bar(center, low_plot, color='firebrick', align='center', width=width, alpha=1, label='low MW')\n",
    "plt.axvline(low_avg, color='firebrick', linestyle='--', linewidth=3)\n",
    "plt.ylim((0,100))\n",
    "plt.xlabel('log $D_{eff}$', fontsize=20)\n",
    "plt.ylabel('Trajectory Count', fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.ylim((0,600))\n",
    "plt.legend(fontsize='large', loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(low_avg, med_avg, high_avg)\n",
    "print(np.exp(low_avg), np.exp(med_avg), np.exp(high_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined HA Solution Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import diff_classifier.aws as aws\n",
    "import math\n",
    "import os\n",
    "import os.path as op\n",
    "from os import listdir\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track = [] # This is going to be the list of all filenames that will be included in the analysis\n",
    "start_knot = 80 #Must be unique number for every run on Cloudknot.\n",
    "\n",
    "remote_folder = '08_06_19_MPT_age_dependence' # The folder in AWS S3 containing the files to be analyzed\n",
    "bucket = 'mckenna.data' # The bucket in AWS S3 where the remote_folder is contained\n",
    "NP_sizes = ['40','100']\n",
    "vids = 5 # this is the number of vids that were taken per condition (usually corresponding to different locations)\n",
    "ages = ['P14', 'P21', 'P28']\n",
    "slices = 3\n",
    "for size in NP_sizes:\n",
    "    for age in ages:\n",
    "        for slic in range(1, slices+1):\n",
    "            for num in range(1, vids+1):\n",
    "                to_track.append('{}_{}nm_s{}_v{}'.format(age, size, slic, num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(to_track))\n",
    "to_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import diff_classifier.aws as aws\n",
    "import math\n",
    "\n",
    "P14_track_40 = to_track[0:15]\n",
    "P21_track_40 = to_track[15:30]\n",
    "P28_track_40 = to_track[30:45]\n",
    "P14_track_100 = to_track[45:60]\n",
    "P21_track_100 = to_track[60:75]\n",
    "P28_track_100 = to_track[75:90]\n",
    "\n",
    "P14_40 = pd.DataFrame()\n",
    "P21_40 = pd.DataFrame()\n",
    "P28_40 = pd.DataFrame()\n",
    "P14_100 = pd.DataFrame()\n",
    "P21_100 = pd.DataFrame()\n",
    "P28_100 = pd.DataFrame()\n",
    "\n",
    "um_px = 0.07\n",
    "fps_40 = 33\n",
    "fps_100 = 100\n",
    "\n",
    "for prefix in P14_track_40:\n",
    "    feat = 'features_{}.csv'.format(prefix)\n",
    "    aws.download_s3(remote_folder+'/'+feat, feat, bucket_name=bucket)\n",
    "    merged = pd.read_csv(feat)\n",
    "    P14_40 = pd.concat([P14_40, merged['Deff1']*um_px*um_px*fps_40/10], axis=0)\n",
    "    os.remove(feat)\n",
    "P14_40.to_csv('P14_40nm.csv', mode='w', index = False)\n",
    "    \n",
    "for prefix in P21_track_40:\n",
    "    feat = 'features_{}.csv'.format(prefix)\n",
    "    aws.download_s3(remote_folder+'/'+feat, feat, bucket_name=bucket)\n",
    "    merged = pd.read_csv(feat)\n",
    "    P21_40 = pd.concat([P21_40, merged['Deff1']*um_px*um_px*fps_40/10], axis=0)\n",
    "    os.remove(feat)\n",
    "P21_40.to_csv('P21_40nm.csv', mode='w', index = False)\n",
    "    \n",
    "for prefix in P28_track_40:\n",
    "    feat = 'features_{}.csv'.format(prefix)\n",
    "    aws.download_s3(remote_folder+'/'+feat, feat, bucket_name=bucket)\n",
    "    merged = pd.read_csv(feat)\n",
    "    P28_40 = pd.concat([P28_40, merged['Deff1']*um_px*um_px*fps_40/10], axis=0)\n",
    "    os.remove(feat)\n",
    "P28_40.to_csv('P28_40nm.csv', mode='w', index = False)    \n",
    "    \n",
    "for prefix in P14_track_100:\n",
    "    feat = 'features_{}.csv'.format(prefix)\n",
    "    aws.download_s3(remote_folder+'/'+feat, feat, bucket_name=bucket)\n",
    "    merged = pd.read_csv(feat)\n",
    "    P14_100 = pd.concat([P14_100, merged['Deff1']*um_px*um_px*fps_100/10], axis=0)\n",
    "    os.remove(feat)\n",
    "P14_100.to_csv('P14_100nm.csv', mode='w', index = False)\n",
    "    \n",
    "for prefix in P21_track_100:\n",
    "    feat = 'features_{}.csv'.format(prefix)\n",
    "    aws.download_s3(remote_folder+'/'+feat, feat, bucket_name=bucket)\n",
    "    merged = pd.read_csv(feat)\n",
    "    P21_100 = pd.concat([P21_100, merged['Deff1']*um_px*um_px*fps_100/10], axis=0)\n",
    "    os.remove(feat)\n",
    "P21_100.to_csv('P21_100nm.csv', mode='w', index = False)\n",
    "    \n",
    "for prefix in P28_track_100:\n",
    "    feat = 'features_{}.csv'.format(prefix)\n",
    "    aws.download_s3(remote_folder+'/'+feat, feat, bucket_name=bucket)\n",
    "    merged = pd.read_csv(feat)\n",
    "    P28_100 = pd.concat([P28_100, merged['Deff1']*um_px*um_px*fps_100/10], axis=0)\n",
    "    os.remove(feat)\n",
    "P28_100.to_csv('P28_100nm.csv', mode='w', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track = [] # This is going to be the list of all filenames that will be included in the analysis\n",
    "start_knot = 80 #Must be unique number for every run on Cloudknot.\n",
    "\n",
    "remote_folder = '07_10_19_MPT_HA_solutions_trial2' # The folder in AWS S3 containing the files to be analyzed\n",
    "bucket = 'mckenna.data' # The bucket in AWS S3 where the remote_folder is contained\n",
    "NP_sizes = ['40', '100']\n",
    "vids = 5 # this is the number of vids that were taken per condition (usually corresponding to different locations)\n",
    "mol_weights = ['high', 'low', 'med']\n",
    "\n",
    "for MW in mol_weights:\n",
    "    for size in NP_sizes:\n",
    "        for num in range(1, vids+1):\n",
    "            to_track.append('{}MW_{}nm_vid_{}'.format(MW, size, num))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(to_track))\n",
    "to_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import diff_classifier.aws as aws\n",
    "import math\n",
    "\n",
    "high_track_40 = to_track[0:5]\n",
    "low_track_40 = to_track[10:15]\n",
    "med_track_40 = to_track[20:25]\n",
    "high_track_100 = to_track[5:10]\n",
    "low_track_100 = to_track[15:20]\n",
    "med_track_100 = to_track[25:30]\n",
    "\n",
    "\n",
    "low_MW_40 = pd.DataFrame()\n",
    "med_MW_40 = pd.DataFrame()\n",
    "high_MW_40 = pd.DataFrame()\n",
    "low_MW_100 = pd.DataFrame()\n",
    "med_MW_100 = pd.DataFrame()\n",
    "high_MW_100 = pd.DataFrame()\n",
    "\n",
    "um_px = 0.07\n",
    "fps_40 = 33\n",
    "fps_100 = 100\n",
    "\n",
    "for prefix in low_track_40:\n",
    "    feat = 'features_{}.csv'.format(prefix)\n",
    "    aws.download_s3(remote_folder+'/'+feat, feat, bucket_name=bucket)\n",
    "    merged = pd.read_csv(feat)\n",
    "    low_MW_40 = pd.concat([low_MW_40, merged['Deff1']*um_px*um_px*fps_40/10], axis=0)\n",
    "    os.remove(feat)\n",
    "low_MW_40.to_csv('40nm_lowMW_trial_2.csv', mode='w', index = False)\n",
    "    \n",
    "for prefix in med_track_40:\n",
    "    feat = 'features_{}.csv'.format(prefix)\n",
    "    aws.download_s3(remote_folder+'/'+feat, feat, bucket_name=bucket)\n",
    "    merged = pd.read_csv(feat)\n",
    "    med_MW_40 = pd.concat([med_MW_40, merged['Deff1']*um_px*um_px*fps_40/10], axis=0)\n",
    "    os.remove(feat)\n",
    "med_MW_40.to_csv('40nm_medMW_trial_2.csv', mode='w', index = False)\n",
    "    \n",
    "for prefix in high_track_40:\n",
    "    feat = 'features_{}.csv'.format(prefix)\n",
    "    aws.download_s3(remote_folder+'/'+feat, feat, bucket_name=bucket)\n",
    "    merged = pd.read_csv(feat)\n",
    "    high_MW_40 = pd.concat([high_MW_40, merged['Deff1']*um_px*um_px*fps_40/10], axis=0)\n",
    "    os.remove(feat)\n",
    "high_MW_40.to_csv('40nm_highMW_trial_2.csv', mode='w', index = False)    \n",
    "    \n",
    "for prefix in low_track_100:\n",
    "    feat = 'features_{}.csv'.format(prefix)\n",
    "    aws.download_s3(remote_folder+'/'+feat, feat, bucket_name=bucket)\n",
    "    merged = pd.read_csv(feat)\n",
    "    low_MW_100 = pd.concat([low_MW_100, merged['Deff1']*um_px*um_px*fps_100/10], axis=0)\n",
    "    os.remove(feat)\n",
    "low_MW_100.to_csv('100nm_lowMW_trial_2.csv', mode='w', index = False)\n",
    "    \n",
    "for prefix in med_track_100:\n",
    "    feat = 'features_{}.csv'.format(prefix)\n",
    "    aws.download_s3(remote_folder+'/'+feat, feat, bucket_name=bucket)\n",
    "    merged = pd.read_csv(feat)\n",
    "    med_MW_100 = pd.concat([med_MW_100, merged['Deff1']*um_px*um_px*fps_100/10], axis=0)\n",
    "    os.remove(feat)\n",
    "med_MW_100.to_csv('100nm_medMW_trial_2.csv', mode='w', index = False)\n",
    "    \n",
    "for prefix in high_track_100:\n",
    "    feat = 'features_{}.csv'.format(prefix)\n",
    "    aws.download_s3(remote_folder+'/'+feat, feat, bucket_name=bucket)\n",
    "    merged = pd.read_csv(feat)\n",
    "    high_MW_100 = pd.concat([high_MW_100, merged['Deff1']*um_px*um_px*fps_100/10], axis=0)\n",
    "    os.remove(feat)\n",
    "high_MW_100.to_csv('100nm_highMW_trial_2.csv', mode='w', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "P14_40nm = pd.read_csv('P14_40nm.csv')\n",
    "P21_40nm = pd.read_csv('P21_40nm.csv')\n",
    "P28_40nm = pd.read_csv('P28_40nm.csv')\n",
    "NT_brain_2 = pd.read_csv('NT_brain_2_hist.csv')\n",
    "\n",
    "P35_40nm = pd.DataFrame()\n",
    "P35_40nm = pd.concat([NT_brain_2], axis=0)\n",
    "\n",
    "P14_40_no_nan = P14_40nm.replace(0,np.nan)\n",
    "P21_40_no_nan = P21_40nm.replace(0,np.nan)\n",
    "P28_40_no_nan = P28_40nm.replace(0,np.nan)\n",
    "P35_40_no_nan = P35_40nm.replace(0,np.nan)\n",
    "\n",
    "log_Deff_P14_40 = np.log(P14_40_no_nan.dropna())\n",
    "log_Deff_P21_40 = np.log(P21_40_no_nan.dropna())\n",
    "log_Deff_P28_40 = np.log(P28_40_no_nan.dropna())\n",
    "log_Deff_P35_40 = np.log(P35_40_no_nan.dropna())\n",
    "\n",
    "test_bins = np.linspace(-10, 2, 76)\n",
    "\n",
    "P14_hist_40, P14_bins_40 = np.histogram(log_Deff_P14_40, bins=test_bins)\n",
    "P21_hist_40, P21_bins_40 = np.histogram(log_Deff_P21_40, bins=test_bins)\n",
    "P28_hist_40, P28_bins_40 = np.histogram(log_Deff_P28_40, bins=test_bins)\n",
    "P35_hist_40, P35_bins_40 = np.histogram(log_Deff_P35_40, bins=test_bins)\n",
    "\n",
    "P14_avg_40 = np.mean(log_Deff_P14_40)[0]\n",
    "P21_avg_40 = np.mean(log_Deff_P21_40)[0]\n",
    "P28_avg_40 = np.mean(log_Deff_P28_40)[0]\n",
    "P35_avg_40 = np.mean(log_Deff_P35_40)[0]\n",
    "\n",
    "plt.rc('axes', linewidth=2)\n",
    "P14_plot_40, P21_plot_40, P28_plot_40, P35_plot_40 = P14_hist_40, P21_hist_40, P28_hist_40, P35_hist_40\n",
    "bins = test_bins\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:])/2\n",
    "\n",
    "plt.figure(1, figsize=(7,4)).tight_layout()\n",
    "plt.bar(center, P14_plot_40, color='grey', align='center', width=width, alpha=0.8, label='P14')\n",
    "plt.axvline(P14_avg_40, color='grey', linestyle='--', linewidth=3)\n",
    "plt.ylim((0,800))\n",
    "plt.xlabel('log $D_{eff}$', fontsize=20)\n",
    "plt.ylabel('Trajectory Count', fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.figure(1, figsize=(7,4)).tight_layout()\n",
    "plt.bar(center, P21_plot_40, color='goldenrod', align='center', width=width, alpha=0.7, label='P21')\n",
    "plt.axvline(P21_avg_40, color='goldenrod', linestyle='--', linewidth=3)\n",
    "plt.ylim((0,400))\n",
    "plt.xlabel('log $D_{eff}$', fontsize=20)\n",
    "plt.ylabel('Trajectory Count', fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.figure(1, figsize=(7,4)).tight_layout()\n",
    "plt.bar(center, P28_plot_40, color='royalblue', align='center', width=width, alpha=0.7, label='P28')\n",
    "plt.axvline(P28_avg_40, color='royalblue', linestyle='--', linewidth=3)\n",
    "plt.ylim((0,100))\n",
    "plt.xlabel('log $D_{eff}$', fontsize=20)\n",
    "plt.ylabel('Trajectory Count', fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.figure(1, figsize=(7,4)).tight_layout()\n",
    "plt.bar(center, P35_plot_40, color='purple', align='center', width=width, alpha=0.7, label='P35')\n",
    "plt.axvline(P35_avg_40, color='purple', linestyle='--', linewidth=3)\n",
    "plt.ylim((0,100))\n",
    "plt.xlabel('log $D_{eff}$', fontsize=20)\n",
    "plt.ylabel('Trajectory Count', fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.ylim((0,1400))\n",
    "plt.legend(fontsize='x-large', loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import diff_classifier.aws as aws\n",
    "import math\n",
    "import os\n",
    "import os.path as op\n",
    "from os import listdir\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track[0:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "um_px = 0.07\n",
    "fps = 33\n",
    "\n",
    "for prefix in to_track[0:45]:\n",
    "    temp = pd.DataFrame()\n",
    "    feat = 'features_{}.csv'.format(prefix)\n",
    "    aws.download_s3(remote_folder+'/'+feat, feat, bucket_name=bucket)\n",
    "    merged = pd.read_csv(feat)\n",
    "    temp = pd.concat([temp, merged['Deff1']*um_px*um_px/(1/fps*10)], axis=0)\n",
    "    temp_no_nan = temp.replace(0,np.nan)\n",
    "    temp_upload = temp_no_nan.dropna()\n",
    "    temp_upload.to_csv('{}_stats.csv'.format(prefix), mode='w', index = False)\n",
    "    aws.upload_s3('{}_stats.csv'.format(prefix), '07_16_19_ECM_Breakdown/{}_stats.csv'.format(prefix), bucket_name='mckenna.data')\n",
    "    os.remove(feat)\n",
    "    os.remove('{}_stats.csv'.format(prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(P14_avg_40, P21_avg_40, P28_avg_40, P35_avg_40)\n",
    "print(np.exp(P14_avg_40), np.exp(P21_avg_40), np.exp(P28_avg_40), np.exp(P35_avg_40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "P14_40nm = pd.read_csv('P14_40nm.csv')\n",
    "P21_40nm = pd.read_csv('P21_40nm.csv')\n",
    "P28_40nm = pd.read_csv('P28_40nm.csv')\n",
    "NT_brain_2 = pd.read_csv('NT_brain_2_hist.csv')\n",
    "\n",
    "P35_40nm = pd.DataFrame()\n",
    "P35_40nm = pd.concat([NT_brain_2], axis=0)\n",
    "\n",
    "P14_40_no_nan = P14_40nm.replace(0,np.nan)\n",
    "P21_40_no_nan = P21_40nm.replace(0,np.nan)\n",
    "P28_40_no_nan = P28_40nm.replace(0,np.nan)\n",
    "P35_40_no_nan = P35_40nm.replace(0,np.nan)\n",
    "\n",
    "log_Deff_P14_40 = np.log(P14_40_no_nan.dropna())\n",
    "log_Deff_P21_40 = np.log(P21_40_no_nan.dropna())\n",
    "log_Deff_P28_40 = np.log(P28_40_no_nan.dropna())\n",
    "log_Deff_P35_40 = np.log(P35_40_no_nan.dropna())\n",
    "\n",
    "test_bins = np.linspace(-10, 2, 76)\n",
    "\n",
    "P14_hist_40, P14_bins_40 = np.histogram(log_Deff_P14_40, bins=test_bins)\n",
    "P21_hist_40, P21_bins_40 = np.histogram(log_Deff_P21_40, bins=test_bins)\n",
    "P28_hist_40, P28_bins_40 = np.histogram(log_Deff_P28_40, bins=test_bins)\n",
    "P35_hist_40, P35_bins_40 = np.histogram(log_Deff_P35_40, bins=test_bins)\n",
    "\n",
    "P14_avg_40 = np.mean(log_Deff_P14_40)[0]\n",
    "P21_avg_40 = np.mean(log_Deff_P21_40)[0]\n",
    "P28_avg_40 = np.mean(log_Deff_P28_40)[0]\n",
    "P35_avg_40 = np.mean(log_Deff_P35_40)[0]\n",
    "\n",
    "plt.rc('axes', linewidth=2)\n",
    "P14_plot_40, P21_plot_40, P28_plot_40, P35_plot_40 = P14_hist_40, P21_hist_40, P28_hist_40, P35_hist_40\n",
    "bins = test_bins\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:])/2\n",
    "\n",
    "plt.figure(1, figsize=(7,4)).tight_layout()\n",
    "plt.bar(center, P21_plot_40, color='goldenrod', align='center', width=width, alpha=0.7, label='P21')\n",
    "plt.axvline(P21_avg_40, color='goldenrod', linestyle='--', linewidth=3)\n",
    "plt.ylim((0,400))\n",
    "plt.xlabel('log $D_{eff}$', fontsize=20)\n",
    "plt.ylabel('Trajectory Count', fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.ylim((0,1400))\n",
    "plt.legend(fontsize='x-large', loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "P14_40nm = pd.read_csv('P14_40nm.csv')\n",
    "P21_40nm = pd.read_csv('P21_40nm.csv')\n",
    "P28_40nm = pd.read_csv('P28_40nm.csv')\n",
    "NT_brain_2 = pd.read_csv('NT_brain_2_hist.csv')\n",
    "\n",
    "P35_40nm = pd.DataFrame()\n",
    "P35_40nm = pd.concat([NT_brain_2], axis=0)\n",
    "\n",
    "P14_40_no_nan = P14_40nm.replace(0,np.nan)\n",
    "P21_40_no_nan = P21_40nm.replace(0,np.nan)\n",
    "P28_40_no_nan = P28_40nm.replace(0,np.nan)\n",
    "P35_40_no_nan = P35_40nm.replace(0,np.nan)\n",
    "\n",
    "log_Deff_P14_40 = np.log(P14_40_no_nan.dropna())\n",
    "log_Deff_P21_40 = np.log(P21_40_no_nan.dropna())\n",
    "log_Deff_P28_40 = np.log(P28_40_no_nan.dropna())\n",
    "log_Deff_P35_40 = np.log(P35_40_no_nan.dropna())\n",
    "\n",
    "test_bins = np.linspace(-10, 2, 76)\n",
    "\n",
    "P14_hist_40, P14_bins_40 = np.histogram(log_Deff_P14_40, bins=test_bins)\n",
    "P21_hist_40, P21_bins_40 = np.histogram(log_Deff_P21_40, bins=test_bins)\n",
    "P28_hist_40, P28_bins_40 = np.histogram(log_Deff_P28_40, bins=test_bins)\n",
    "P35_hist_40, P35_bins_40 = np.histogram(log_Deff_P35_40, bins=test_bins)\n",
    "\n",
    "P14_avg_40 = np.mean(log_Deff_P14_40)[0]\n",
    "P21_avg_40 = np.mean(log_Deff_P21_40)[0]\n",
    "P28_avg_40 = np.mean(log_Deff_P28_40)[0]\n",
    "P35_avg_40 = np.mean(log_Deff_P35_40)[0]\n",
    "\n",
    "plt.rc('axes', linewidth=2)\n",
    "P14_plot_40, P21_plot_40, P28_plot_40, P35_plot_40 = P14_hist_40, P21_hist_40, P28_hist_40, P35_hist_40\n",
    "bins = test_bins\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:])/2\n",
    "\n",
    "plt.figure(1, figsize=(7,4)).tight_layout()\n",
    "plt.bar(center, P35_plot_40, color='purple', align='center', width=width, alpha=0.7, label='P35')\n",
    "plt.axvline(P35_avg_40, color='purple', linestyle='--', linewidth=3)\n",
    "plt.ylim((0,100))\n",
    "plt.xlabel('log $D_{eff}$', fontsize=20)\n",
    "plt.ylabel('Trajectory Count', fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.ylim((0,1400))\n",
    "plt.legend(fontsize='x-large', loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "P14_40nm = pd.read_csv('P14_40nm.csv')\n",
    "P21_40nm = pd.read_csv('P21_40nm.csv')\n",
    "P28_40nm = pd.read_csv('P28_40nm.csv')\n",
    "NT_brain_2 = pd.read_csv('NT_brain_2_hist.csv')\n",
    "\n",
    "P35_40nm = pd.DataFrame()\n",
    "P35_40nm = pd.concat([NT_brain_2], axis=0)\n",
    "\n",
    "P14_40_no_nan = P14_40nm.replace(0,np.nan)\n",
    "P21_40_no_nan = P21_40nm.replace(0,np.nan)\n",
    "P28_40_no_nan = P28_40nm.replace(0,np.nan)\n",
    "P35_40_no_nan = P35_40nm.replace(0,np.nan)\n",
    "\n",
    "log_Deff_P14_40 = np.log(P14_40_no_nan.dropna())\n",
    "log_Deff_P21_40 = np.log(P21_40_no_nan.dropna())\n",
    "log_Deff_P28_40 = np.log(P28_40_no_nan.dropna())\n",
    "log_Deff_P35_40 = np.log(P35_40_no_nan.dropna())\n",
    "\n",
    "test_bins = np.linspace(-10, 2, 76)\n",
    "\n",
    "P14_hist_40, P14_bins_40 = np.histogram(log_Deff_P14_40, bins=test_bins)\n",
    "P21_hist_40, P21_bins_40 = np.histogram(log_Deff_P21_40, bins=test_bins)\n",
    "P28_hist_40, P28_bins_40 = np.histogram(log_Deff_P28_40, bins=test_bins)\n",
    "P35_hist_40, P35_bins_40 = np.histogram(log_Deff_P35_40, bins=test_bins)\n",
    "\n",
    "P14_avg_40 = np.mean(log_Deff_P14_40)[0]\n",
    "P21_avg_40 = np.mean(log_Deff_P21_40)[0]\n",
    "P28_avg_40 = np.mean(log_Deff_P28_40)[0]\n",
    "P35_avg_40 = np.mean(log_Deff_P35_40)[0]\n",
    "\n",
    "plt.rc('axes', linewidth=2)\n",
    "P14_plot_40, P21_plot_40, P28_plot_40, P35_plot_40 = P14_hist_40, P21_hist_40, P28_hist_40, P35_hist_40\n",
    "bins = test_bins\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:])/2\n",
    "\n",
    "plt.figure(1, figsize=(7,4)).tight_layout()\n",
    "plt.bar(center, P14_plot_40, color='grey', align='center', width=width, alpha=0.8, label='P14')\n",
    "plt.axvline(P14_avg_40, color='grey', linestyle='--', linewidth=3)\n",
    "plt.ylim((0,800))\n",
    "plt.xlabel('log $D_{eff}$', fontsize=20)\n",
    "plt.ylabel('Trajectory Count', fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.ylim((0,1400))\n",
    "plt.legend(fontsize='x-large', loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "P14_40nm = pd.read_csv('P14_40nm.csv')\n",
    "P21_40nm = pd.read_csv('P21_40nm.csv')\n",
    "P28_40nm = pd.read_csv('P28_40nm.csv')\n",
    "NT_brain_2 = pd.read_csv('NT_brain_2_hist.csv')\n",
    "\n",
    "P35_40nm = pd.DataFrame()\n",
    "P35_40nm = pd.concat([NT_brain_2], axis=0)\n",
    "\n",
    "P14_40_no_nan = P14_40nm.replace(0,np.nan)\n",
    "P21_40_no_nan = P21_40nm.replace(0,np.nan)\n",
    "P28_40_no_nan = P28_40nm.replace(0,np.nan)\n",
    "P35_40_no_nan = P35_40nm.replace(0,np.nan)\n",
    "\n",
    "log_Deff_P14_40 = np.log(P14_40_no_nan.dropna())\n",
    "log_Deff_P21_40 = np.log(P21_40_no_nan.dropna())\n",
    "log_Deff_P28_40 = np.log(P28_40_no_nan.dropna())\n",
    "log_Deff_P35_40 = np.log(P35_40_no_nan.dropna())\n",
    "\n",
    "test_bins = np.linspace(-10, 2, 76)\n",
    "\n",
    "P14_hist_40, P14_bins_40 = np.histogram(log_Deff_P14_40, bins=test_bins)\n",
    "P21_hist_40, P21_bins_40 = np.histogram(log_Deff_P21_40, bins=test_bins)\n",
    "P28_hist_40, P28_bins_40 = np.histogram(log_Deff_P28_40, bins=test_bins)\n",
    "P35_hist_40, P35_bins_40 = np.histogram(log_Deff_P35_40, bins=test_bins)\n",
    "\n",
    "P14_avg_40 = np.mean(log_Deff_P14_40)[0]\n",
    "P21_avg_40 = np.mean(log_Deff_P21_40)[0]\n",
    "P28_avg_40 = np.mean(log_Deff_P28_40)[0]\n",
    "P35_avg_40 = np.mean(log_Deff_P35_40)[0]\n",
    "\n",
    "plt.rc('axes', linewidth=2)\n",
    "P14_plot_40, P21_plot_40, P28_plot_40, P35_plot_40 = P14_hist_40, P21_hist_40, P28_hist_40, P35_hist_40\n",
    "bins = test_bins\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:])/2\n",
    "\n",
    "plt.figure(1, figsize=(7,4)).tight_layout()\n",
    "plt.bar(center, P28_plot_40, color='royalblue', align='center', width=width, alpha=0.7, label='P28')\n",
    "plt.axvline(P28_avg_40, color='royalblue', linestyle='--', linewidth=3)\n",
    "plt.ylim((0,100))\n",
    "plt.xlabel('log $D_{eff}$', fontsize=20)\n",
    "plt.ylabel('Trajectory Count', fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.ylim((0,1400))\n",
    "plt.legend(fontsize='x-large', loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "P14_100nm = pd.read_csv('P14_100nm.csv')\n",
    "P21_100nm = pd.read_csv('P21_100nm.csv')\n",
    "P28_100nm = pd.read_csv('P28_100nm.csv')\n",
    "#NT_brain_2 = pd.read_csv('NT_brain_2_hist.csv')\n",
    "\n",
    "#P35_40nm = pd.DataFrame()\n",
    "#P35_40nm = pd.concat([NT_brain_2], axis=0)\n",
    "\n",
    "P14_100_no_nan = P14_100nm.replace(0,np.nan)\n",
    "P21_100_no_nan = P21_100nm.replace(0,np.nan)\n",
    "P28_100_no_nan = P28_100nm.replace(0,np.nan)\n",
    "#P35_40_no_nan = P35_40nm.replace(0,np.nan)\n",
    "\n",
    "log_Deff_P14_100 = np.log(P14_100_no_nan.dropna())\n",
    "log_Deff_P21_100 = np.log(P21_100_no_nan.dropna())\n",
    "log_Deff_P28_100 = np.log(P28_100_no_nan.dropna())\n",
    "#log_Deff_P35_40 = np.log(P35_40_no_nan.dropna())\n",
    "\n",
    "test_bins = np.linspace(-5, 1, 76)\n",
    "\n",
    "P14_hist_100, P14_bins_100 = np.histogram(log_Deff_P14_100, bins=test_bins)\n",
    "P21_hist_100, P21_bins_100 = np.histogram(log_Deff_P21_100, bins=test_bins)\n",
    "P28_hist_100, P28_bins_100 = np.histogram(log_Deff_P28_100, bins=test_bins)\n",
    "#P35_hist_40, P35_bins_40 = np.histogram(log_Deff_P35_40, bins=test_bins)\n",
    "\n",
    "P14_avg_100 = np.mean(log_Deff_P14_100)[0]\n",
    "P21_avg_100 = np.mean(log_Deff_P21_100)[0]\n",
    "P28_avg_100 = np.mean(log_Deff_P28_100)[0]\n",
    "#P35_avg_40 = np.mean(log_Deff_P35_40)[0]\n",
    "\n",
    "plt.rc('axes', linewidth=2)\n",
    "P14_plot_100, P21_plot_100, P28_plot_100 = P14_hist_100, P21_hist_100, P28_hist_100\n",
    "bins = test_bins\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:])/2\n",
    "\n",
    "plt.figure(1, figsize=(7,4)).tight_layout()\n",
    "plt.bar(center, P14_plot_100, color='royalblue', align='center', width=width, alpha=0.8, label='P14')\n",
    "plt.axvline(P14_avg_100, color='royalblue', linestyle='--', linewidth=3)\n",
    "plt.ylim((0,800))\n",
    "plt.xlabel('log $D_{eff}$', fontsize=20)\n",
    "plt.ylabel('Trajectory Count', fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.figure(1, figsize=(7,4)).tight_layout()\n",
    "plt.bar(center, P21_plot_100, color='darkgreen', align='center', width=width, alpha=0.9, label='P21')\n",
    "plt.axvline(P21_avg_100, color='darkgreen', linestyle='--', linewidth=3)\n",
    "plt.ylim((0,400))\n",
    "plt.xlabel('log $D_{eff}$', fontsize=20)\n",
    "plt.ylabel('Trajectory Count', fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.figure(1, figsize=(7,4)).tight_layout()\n",
    "plt.bar(center, P28_plot_100, color='firebrick', align='center', width=width, alpha=0.7, label='P28')\n",
    "plt.axvline(P28_avg_100, color='firebrick', linestyle='--', linewidth=3)\n",
    "plt.ylim((0,100))\n",
    "plt.xlabel('log $D_{eff}$', fontsize=20)\n",
    "plt.ylabel('Trajectory Count', fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.ylim((0,2000))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(P14_avg_100, P21_avg_100, P28_avg_100)\n",
    "print(np.exp(P14_avg_100), np.exp(P21_avg_100), np.exp(P28_avg_100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSD plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import diff_classifier.aws as aws\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prefix in to_track[0:45]:\n",
    "    filename = 'adj_geomean_{}.csv'.format(prefix)\n",
    "    aws.download_s3(remote_folder+'/'+filename, filename, bucket_name=bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prefix in to_track[0:45]:\n",
    "    filename = 'adj_geomean_{}.csv'.format(prefix)\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is importing P35 dataset\n",
    "\n",
    "to_track_P35 = [] # This is going to be the list of all filenames that will be included in the analysis\n",
    "start_knot = 80 #Must be unique number for every run on Cloudknot.\n",
    "\n",
    "remote_folder_P35 = '07_16_19_MPT_ECM_breakdown' # The folder in AWS S3 containing the files to be analyzed\n",
    "bucket = 'mckenna.data' # The bucket in AWS S3 where the remote_folder is contained\n",
    "\n",
    "treatments = ['NT']\n",
    "brains = 4\n",
    "slices = 3\n",
    "vids = 5\n",
    "\n",
    "for treat in treatments:\n",
    "    for brain in range(1, brains+1):\n",
    "        for slic in range(1, slices+1):\n",
    "            for num in range(1, vids+1):\n",
    "                to_track_P35.append('{}_brain_{}_slice_{}_vid_{}'.format(treat, brain, slic, num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_track_P35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "P14_MSDs = pd.DataFrame()\n",
    "P21_MSDs = pd.DataFrame()\n",
    "P28_MSDs = pd.DataFrame()\n",
    "P35_MSDs = pd.DataFrame()\n",
    "\n",
    "P14_plot_array = []\n",
    "P21_plot_array = []\n",
    "P28_plot_array = []\n",
    "P35_plot_array = []\n",
    "\n",
    "P14_MSDs['time'] = pd.Series(np.linspace(1/33, 1/33*650, 650))\n",
    "P21_MSDs['time'] = pd.Series(np.linspace(1/33, 1/33*650, 650))\n",
    "P28_MSDs['time'] = pd.Series(np.linspace(1/33, 1/33*650, 650))\n",
    "P35_MSDs['time'] = pd.Series(np.linspace(1/33, 1/33*650, 650))\n",
    "\n",
    "for prefix in to_track[0:15]:\n",
    "    filename = 'adj_geomean_{}.csv'.format(prefix)\n",
    "    P14_MSDs = pd.concat([P14_MSDs, pd.read_csv(filename)['exp']], axis = 1)\n",
    "    P14_MSDs.rename(columns={\"exp\": prefix}, inplace=True)\n",
    "    P14_plot_array.append(prefix)\n",
    "\n",
    "P14_MSDs['average'] = P14_MSDs.mean(numeric_only=True, axis=1)\n",
    "\n",
    "for prefix in to_track[15:30]:\n",
    "    filename = 'adj_geomean_{}.csv'.format(prefix)\n",
    "    P21_MSDs = pd.concat([P21_MSDs, pd.read_csv(filename)['exp']], axis = 1)\n",
    "    P21_MSDs.rename(columns={\"exp\": prefix}, inplace=True)\n",
    "    P21_plot_array.append(prefix)\n",
    "    \n",
    "P21_MSDs['average'] = P21_MSDs.mean(numeric_only=True, axis=1)\n",
    "\n",
    "for prefix in to_track[30:45]:\n",
    "    filename = 'adj_geomean_{}.csv'.format(prefix)\n",
    "    P28_MSDs = pd.concat([P28_MSDs, pd.read_csv(filename)['exp']], axis = 1)\n",
    "    P28_MSDs.rename(columns={\"exp\": prefix}, inplace=True)\n",
    "    P28_plot_array.append(prefix)\n",
    "\n",
    "P28_MSDs['average'] = P28_MSDs.mean(numeric_only=True, axis=1)\n",
    "\n",
    "for prefix in to_track_P35:\n",
    "    filename = 'adj_geomean_{}.csv'.format(prefix)\n",
    "    P35_MSDs = pd.concat([P35_MSDs, pd.read_csv(filename)['exp']], axis = 1)\n",
    "    P35_MSDs.rename(columns={\"exp\": prefix}, inplace=True)\n",
    "    P35_plot_array.append(prefix)\n",
    "\n",
    "P35_MSDs['average'] = P35_MSDs.mean(numeric_only=True, axis=1)\n",
    "\n",
    "plt.figure(1).tight_layout()\n",
    "ax = plt.gca()\n",
    "P14_MSDs.plot(x='time', y=P14_plot_array, kind='line', ax=ax, xlim=(0,1.6), ylim=(0.01,100), logy=True, logx=True, color='gray', alpha=0.1, legend=False)\n",
    "P14_MSDs.plot(x='time', y='average', kind='line', ax=ax, xlim=(0,1.6), ylim=(0.01,100), logy=True, logx=True, linewidth=3.0, color='gray', alpha=1, legend=False)\n",
    "ax.set_ylabel('<MSD> (\\u03BCm$^2$)', fontsize=18)\n",
    "ax.set_xlabel('lag time (s)', fontsize=18)\n",
    "ax.tick_params(labelsize=12)\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    ax.spines[axis].set_linewidth(2)\n",
    "\n",
    "plt.figure(2).tight_layout()\n",
    "ax2 = plt.gca()\n",
    "P21_MSDs.plot(x='time', y=P21_plot_array, kind='line', ax=ax2, xlim=(0,1.6), ylim=(0.01,100), logy=True, logx=True, color='goldenrod', alpha=0.1, legend=False)\n",
    "P21_MSDs.plot(x='time', y='average', kind='line', ax=ax2, xlim=(0,1.6), ylim=(0.01,100), logy=True, logx=True, linewidth=3.0, color='goldenrod', alpha=1, legend=False)\n",
    "ax2.set_ylabel('<MSD> (\\u03BCm$^2$)', fontsize=18)\n",
    "ax2.set_xlabel('lag time (s)', fontsize=18)\n",
    "ax2.tick_params(labelsize=12)\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    ax2.spines[axis].set_linewidth(2)\n",
    "\n",
    "plt.figure(3).tight_layout()\n",
    "ax3 = plt.gca()\n",
    "P28_MSDs.plot(x='time', y=P28_plot_array, kind='line', ax=ax3, xlim=(0,1.6), logy=True, ylim=(0.01,100), logx=True, color='royalblue', alpha=0.1, legend=False)\n",
    "P28_MSDs.plot(x='time', y='average', kind='line', ax=ax3, xlim=(0,1.6), ylim=(0.01,100), logy=True, logx=True, linewidth=3.0, color='royalblue', alpha=1, legend=False)\n",
    "ax3.set_ylabel('<MSD> (\\u03BCm$^2$)', fontsize=18)\n",
    "ax3.set_xlabel('lag time (s)', fontsize=18)\n",
    "ax3.tick_params(labelsize=12)\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    ax3.spines[axis].set_linewidth(2)\n",
    "    \n",
    "plt.figure(4).tight_layout()\n",
    "ax4 = plt.gca()\n",
    "P35_MSDs.plot(x='time', y=P35_plot_array, kind='line', ax=ax4, xlim=(0,1.6), logy=True, ylim=(0.01,100), logx=True, color='purple', alpha=0.1, legend=False)\n",
    "P35_MSDs.plot(x='time', y='average', kind='line', ax=ax4, xlim=(0,1.6), ylim=(0.01,100), logy=True, logx=True, linewidth=3.0, color='purple', alpha=1, legend=False)\n",
    "ax4.set_ylabel('<MSD> (\\u03BCm$^2$)', fontsize=18)\n",
    "ax4.set_xlabel('lag time (s)', fontsize=18)\n",
    "ax4.tick_params(labelsize=12)\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    ax4.spines[axis].set_linewidth(2)\n",
    "\n",
    "plt.figure(5).tight_layout()\n",
    "ax5 = plt.gca()\n",
    "P14_MSDs.plot(x='time', y=P14_plot_array, kind='line', ax=ax5, xlim=(0,1.6), ylim=(0.01,100), logy=True, logx=True, color='gray', alpha=0.05, legend=False)\n",
    "P14_MSDs.plot(x='time', y='average', kind='line', ax=ax5, xlim=(0,1.6), ylim=(0.01,100), logy=True, logx=True, linewidth=4.0, color='gray', alpha=1, label='P14')\n",
    "P21_MSDs.plot(x='time', y=P21_plot_array, kind='line', ax=ax5, xlim=(0,1.6), ylim=(0.01,100), logy=True, logx=True, color='goldenrod', alpha=0.05, legend=False)\n",
    "P21_MSDs.plot(x='time', y='average', kind='line', ax=ax5, xlim=(0,1.6), ylim=(0.01,100), logy=True, logx=True, linewidth=4.0, color='goldenrod', alpha=1, label='P21')\n",
    "P28_MSDs.plot(x='time', y=P28_plot_array, kind='line', ax=ax5, xlim=(0,1.6), logy=True, ylim=(0.01,100), logx=True, color='royalblue', alpha=0.05, legend=False)\n",
    "P28_MSDs.plot(x='time', y='average', kind='line', ax=ax5, xlim=(0,1.6), ylim=(0.01,100), logy=True, logx=True, linewidth=4.0, color='royalblue', alpha=1, label='P28')\n",
    "P35_MSDs.plot(x='time', y=P35_plot_array, kind='line', ax=ax5, xlim=(0,1.6), logy=True, ylim=(0.01,100), logx=True, color='purple', alpha=0.05, legend=False)\n",
    "P35_MSDs.plot(x='time', y='average', kind='line', ax=ax5, xlim=(0,1.6), ylim=(0.01,100), logy=True, logx=True, linewidth=4.0, color='purple', alpha=1, label='P35')\n",
    "ax5.set_ylabel('<MSD> (\\u03BCm$^2$)', fontsize=18)\n",
    "ax5.set_xlabel('lag time (s)', fontsize=18)\n",
    "ax5.tick_params(labelsize=12)\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    ax5.spines[axis].set_linewidth(2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
